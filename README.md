# pymongo-api

## Задание 1. Планирование

### Шаг 1. Шардирование в MongoDB

Для повышения производительности в MongoDB используется **шардирование** (горизонтальное партиционирование).  
В рамках данной практики задействованы два шарда. Метод шардирования выбран **хешированный** (Hash Sharding).  
Хеш-функция распределяет данные таким образом, чтобы каждая запись попадала на один из шаров на основе вычисленного хеш-значения.

**Схема шардирования** (первый вариант) доступен в [Google Drive](https://drive.google.com/file/d/1pNo0Ynpk6ZjZftUZAoFlRIByqkp8VaI3/view?usp=share_link) на странице `sharding`.

### Шаг 2. Репликация в MongoDB

Для повышения отказоустойчивости в каждом шарде применяется **репликация** (Replica Set).
- **Первичный (master)** узел отвечает за все операции записи и управление данными.
- **Вторичный (slave)** узел пассивно реплицирует данные от первичного узла и может обслуживать запросы на чтение.
- MongoDB использует heartbeat-сообщения для мониторинга состояния серверов и автоматического восстановления после сбоев.

Для полноценного шардированного решения необходимы дополнительные сервисы:

- **Router (router01)**: определяет, на какой шард будет направлен запрос.
- **Config-сервер (configSrv)**: хранит метаданные кластера (маппинг данных кластера на шарды).

**Схема шардирования с репликацией** (второй вариант) доступен в [Google Drive](https://drive.google.com/file/d/1pNo0Ynpk6ZjZftUZAoFlRIByqkp8VaI3/view?usp=share_link) на странице `replication`.

### Шаг 3. Кэширование с помощью Redis

Для дальнейшего повышения производительности используется **кэширование** по паттерну **Cache Aside**. В качестве сервиса кэширования выбран Redis, поскольку он предоставляет:
- Высокую скорость чтения данных из памяти
- Возможность репликации и шардирования (разделения данных на сегменты)

По условиям задачи приложение рассчитано на использование Redis в Single Node режиме. Кластерный вариант REDIS_URL не поддерживается поэтому **Redis** в нашем проекте состоит из одного сегментов

Но в реальных проектах использовался бы **Redis Cluster** состоящий из трёх сегментов (минимум для обеспечения устойчивой кластеризации). У каждого сегмента:
- **Master** узел (M1, M2, M3) отвечает за записи
- **Slave** узел (S1, S2, S3) хранит копии данных (репликация) и предоставляет операции чтения

При этом:
- Клиенты пишут/читают на **Master**-узлах
- На **Slave**-узлах доступна только операция чтения
- Состояние кластера координируется через протокол gossip

**Схема с кэшированием** (третий вариант) доступен в [Google Drive](https://drive.google.com/file/d/1pNo0Ynpk6ZjZftUZAoFlRIByqkp8VaI3/view?usp=share_link) на странице `caching`.

---

## Задание 2. Шардирование

Для реализации **первого шага** (шардирование в MongoDB) подготовлен файл `compose.yaml`, модифицированный на базе примера из урока по шардированию.

### Как запустить

1. Из папки с проектом перейти в папку `mongo-sharding`:
   ```bash
   cd mongo-sharding
   ```
2. Запустить MongoDB и приложение:
   ```bash
   docker compose up -d
   ```
3. Заполнить MongoDB тестовыми данными:
   ```bash
   ./scripts/mongo-init.sh
   ```
   Внутри `mongo-init.sh` происходит:
    - Подключение к серверу конфигурации и начальная инициализация.
    - Инициализация **первого шарда**.
    - Инициализация **второго шарда**.
    - Инициализация **роутера**.
    - Наполнение роутера тестовыми данными, которые в свою очередь распределяются по шардам.

### Как проверить

1. Открыть в браузере [http://localhost:8080](http://localhost:8080) — убедиться в доступности приложения.
2. При помощи shell-скрипта проверить количество документов в базе **somedb** (на инстансах `mongos_router`, `shard1` и `shard2`):
   ```bash
   ./scripts/mongo-checking-data.sh
   ```
   общее количество документов будет 1500

---

## Задание 3. Репликация
Для реализации **второго шага** (Репликация в MongoDB) подготовлен файл `compose.yaml`, модифицированный на базе `compose.yaml` из `mongo-sharding`.

### Как запустить

1. Из папки с проектом перейти в папку `mongo-sharding-repl`:
   ```bash
   cd mongo-sharding-repl
   ```
2. Запустить MongoDB и приложение:
   ```bash
   docker compose up -d
   ```
3. Заполнить MongoDB тестовыми данными:
   ```bash
   ./scripts/mongo-init.sh
   ```
   Внутри `mongo-init.sh` происходит:
   - Подключение к серверу конфигурации и начальная инициализация.
   - Инициализация **первой реплики первого шарда**.
   - Инициализация **второй реплики первого шарда**.
   - Инициализация **третьей реплики первого шарда**.
   - Инициализация **первой реплики второго шарда**.
   - Инициализация **второй реплики второго шарда**.
   - Инициализация **третьей реплики второго шарда**.
   - Инициализация **роутера**.
   - Наполнение роутера тестовыми данными, которые в свою очередь распределяются по шардам.

### Как проверить

1. Открыть в браузере [http://localhost:8080](http://localhost:8080) — убедиться в доступности приложения.
2. При помощи shell-скрипта проверить:
   - количество документов в базе **somedb** (на инстансах `mongos_router`, `shard1` и `shard2`)
   - количество реплик в shard1
   - количество реплик в shard2
   ```bash
   ./scripts/mongo-checking-data.sh
   ```
   общее количество документов будет 1500

   общее количество реплик в шардах будет по 3
---

## Задание 4. Кэширование
Для реализации **третьего шага** (Кэширование с помощью Redis) подготовлен файл `compose.yaml`, модифицированный на базе `compose.yaml` из `mongo-sharding-repl`.
По условиям задачи приложение рассчитано на использование Redis в Single Node режиме. Кластерный вариант REDIS_URL не поддерживается поэтому **Redis** в нашем проекте состоит из одного сегментов
### Как запустить

1. Из папки с проектом перейти в папку `sharding-repl-cache`:
   ```bash
   cd sharding-repl-cache
   ```
2. Запустить MongoDB и приложение:
   ```bash
   docker compose up -d
   ```
3. Заполнить MongoDB тестовыми данными:
   ```bash
   ./scripts/mongo-init.sh
   ```
   Внутри `mongo-init.sh` происходит:
   - Подключение к серверу конфигурации и начальная инициализация.
   - Инициализация **первой реплики первого шарда**.
   - Инициализация **второй реплики первого шарда**.
   - Инициализация **третьей реплики первого шарда**.
   - Инициализация **первой реплики второго шарда**.
   - Инициализация **второй реплики второго шарда**.
   - Инициализация **третьей реплики второго шарда**.
   - Инициализация **роутера**.
   - Наполнение роутера тестовыми данными, которые в свою очередь распределяются по шардам.

### Как проверить

1. Открыть в браузере [http://localhost:8080](http://localhost:8080) — убедиться в доступности приложения.
2. При помощи shell-скрипта проверить:
   - количество документов в базе **somedb** (на инстансах `mongos_router`, `shard1` и `shard2`)
   - количество реплик в shard1
   - количество реплик в shard2
   ```bash
   ./scripts/mongo-checking-data.sh
   ```
   общее количество документов будет 1500

   общее количество реплик в шардах будет по 3

   второй и последующие вызовы эндпоинта [Все данные пользователей в БД](http://localhost:8080/helloDoc/users) выполняются <100мс.
---

## Задание 5. Service Discovery и балансировка с API Gateway
Реализация механизма обнаружения сервисов и балансировки.

Для распределения траффика использую APISIX Gateway. Чтобы сообщать APISIX Gateway об изменении количества инстансов буду использовать Hashicorp Consul для Service Discovery.
APISIX Gateway, или API-шлюз, — это современный инструмент с открытым исходным кодом, интегрированный с Consul, продуктом компании HashiCorp. Consul выполняет функции реестра IT-сервисов, позволяя регистрировать и удалять сервисы, а также хранить информацию о них в формате «ключ — значение».

Взаимодействие с API-шлюзом происходит следующим образом: запросы с сайта направляются через шлюз, который выполняет роль балансировщика нагрузки и агрегатора данных. API-шлюз передает запросы к целевым сервисам (инстансам), а список доступных сервисов получает из хранилища Consul с использованием адаптера **consul_kv**.

Модуль **consul_kv** обеспечивает ключевые функции реестра обнаружения сервисов, что позволяет разделить серверную часть и клиентское приложение. Эти функции включают два этапа:

1. **Регистрация контуров**: добавление контуров в реестр, что делает их доступными для обнаружения.
2. **Обнаружение контуров**: предоставление клиентскому приложению информации о маршрутах к зарегистрированным сервисам через реестр.

Такой подход обеспечивает гибкость, удобство управления и масштабируемость.
**Схема Service Discovery и балансировка** (четвертый вариант) доступен в [Google Drive](https://drive.google.com/file/d/1pNo0Ynpk6ZjZftUZAoFlRIByqkp8VaI3/view?usp=share_link) на странице `SD & API Gateway`.
---

## Задание 6. CDN
Настройка сети доставки контента.

Для ускорения доставки статического контента пользователям в разных регионах целесообразно внедрить использование CDN (Content Delivery Network). Основой работы CDN является распределение данных через систему доменных имен (DNS), что позволяет направлять запросы к ближайшим точкам присутствия.

В данном случае будет использоваться метод GeoDNS для реализации интеллектуального управления трафиком. GeoDNS (известный также как solitary traffic directors или global traffic directors) — это технология, которая применяет алгоритмы определения географического положения пользователя на основе его IP-адреса.

Процесс работы выглядит следующим образом:
1. Клиент отправляет запрос на получение контента.
2. DNS-сервер анализирует IP-адрес клиента для определения его географического местоположения.
3. На основании полученных данных система перенаправляет запрос к ближайшей точке присутствия CDN.

Этот подход минимизирует время отклика и повышает производительность за счет оптимального распределения нагрузки, обеспечивая более качественный пользовательский опыт независимо от региона.
**Схема CDN** (пятый вариант) доступна в [Google Drive](https://drive.google.com/file/d/1pNo0Ynpk6ZjZftUZAoFlRIByqkp8VaI3/view?usp=share_link) на странице `CDN`.
--- 